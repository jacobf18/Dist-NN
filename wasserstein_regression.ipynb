{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implement Wasserstein regression using PyTorch. We use PyTorch so that we can utilize the AutoGrad framework to calculate gradients since I do not know a closed-form solution for Wasserstein regression similar to scalar linear regression.\n",
    "\n",
    "If this does not work since some of the operations do not let the gradients pass through such as `torch.unique` or `torch.searchsorted`, we will use SPSA to estimate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # will use for AutoGrad\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor(0).reshape(1), torch.diff(torch.tensor([1,2,3,4,5]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(barycenter):\n",
    "    # This is just the integral from 0 to 1 of the quantile function\n",
    "    x,y = barycenter\n",
    "    x_diff = torch.diff(x)\n",
    "    return torch.sum(torch.mul(x_diff, y[1:])) + torch.mul(x[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6250, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 0.25, 0.5, 0.75, 1]).requires_grad_(True)\n",
    "y = torch.tensor([0, 0.25, 0.5, 0.75, 1]).requires_grad_(True)\n",
    "\n",
    "mean = expectation((x,y))\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 3.], grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def empirical_quantile_function(samples):\n",
    "    \"\"\"\n",
    "    Returns a function that computes the empirical quantile function for the given 1D samples.\n",
    "\n",
    "    Args:\n",
    "        samples (torch.Tensor): A 1D tensor of samples from a distribution. Assumes is sorted.\n",
    "\n",
    "    Returns:\n",
    "        function: A function that takes a tensor of quantiles (q) and returns the corresponding quantile values.\n",
    "    \"\"\"\n",
    "    def quantile_function(q):\n",
    "        \"\"\"\n",
    "        Computes the empirical quantile for the given quantiles.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): A tensor of quantiles (values between 0 and 1).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The corresponding quantile values.\n",
    "        \"\"\"\n",
    "        # Compute the empirical CDF values\n",
    "        n = len(samples)\n",
    "        cdf = torch.arange(1, n + 1, dtype=torch.float32) / n\n",
    "        # Use broadcasting to calculate the Heaviside contributions\n",
    "        heaviside_matrix = torch.heaviside(q.unsqueeze(1) - cdf.unsqueeze(0), torch.tensor(1.0))\n",
    "        # Compute quantile values by summing contributions\n",
    "        quantile_values = heaviside_matrix @ samples\n",
    "\n",
    "        return quantile_values\n",
    "\n",
    "    return quantile_function\n",
    "\n",
    "# Example usage\n",
    "samples = torch.tensor([1.0, 2.0]).requires_grad_(True)\n",
    "quantile_fn = empirical_quantile_function(samples)\n",
    "quantiles = torch.tensor([0.1, 0.5, 1.1])\n",
    "result = quantile_fn(quantiles)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_function(x_points, y_points, x):\n",
    "    # Find the interval that x falls into\n",
    "    idx = torch.searchsorted(x_points, x, right = True) - 1\n",
    "    # If x is before the first point, return the first y\n",
    "    if idx < 0:\n",
    "        return y_points[0]\n",
    "    # If x is beyond the last point, return the last y\n",
    "    elif idx >= len(y_points):\n",
    "        return y_points[-1]\n",
    "    # Otherwise, return the y corresponding to the interval\n",
    "    return y_points[idx]\n",
    "\n",
    "val = step_function(x,y, 0.5)\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 5.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples1 = torch.tensor([1.0, 2.0]).requires_grad_(True)\n",
    "samples2 = torch.tensor([2.0, 3.0]).requires_grad_(True)\n",
    "samples3 = torch.tensor([3.0, 4.0]).requires_grad_(True)\n",
    "\n",
    "q1 = empirical_quantile_function(samples1)\n",
    "q2 = empirical_quantile_function(samples2)\n",
    "q3 = empirical_quantile_function(samples3)\n",
    "\n",
    "def linear_combination(quantile_fns, weights):\n",
    "    def lin_comb_fn(q):\n",
    "        # Compute the quantile values for each function\n",
    "        quantile_values = torch.stack([fn(q) for fn in quantile_fns])\n",
    "        # Compute the weighted sum of quantile values\n",
    "        lin_comb_values = torch.sum(weights.unsqueeze(1) * quantile_values, dim=0)\n",
    "\n",
    "        return lin_comb_values\n",
    "\n",
    "    return lin_comb_fn\n",
    "\n",
    "# Example usage\n",
    "quantile_fns = [q1, q2, q3]\n",
    "weights = torch.tensor([1/3, 1/3, 1/3])\n",
    "barycenter_fn = linear_combination(quantile_fns, weights)\n",
    "\n",
    "quantiles = torch.tensor([0.1, 0.5, 1.0])\n",
    "barycenter_fn(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(barycenter_fn(quantiles)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0312, grad_fn=<AddBackward0>)\n",
      "tensor(0.0312, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def squared_difference_integral(x1, y1, x2, y2):\n",
    "    # Combine and sort all x points from both arrays, ensuring no duplicates\n",
    "    x_points = torch.unique_consecutive(torch.cat([x1, x2]))\n",
    "    total_integral = 0.0\n",
    "\n",
    "    for i in range(len(x_points) - 1):\n",
    "        # Define the interval [x_points[i], x_points[i+1]]\n",
    "        x_left = x_points[i]\n",
    "        x_right = x_points[i + 1]\n",
    "        interval_length = x_right - x_left\n",
    "\n",
    "        # Find the y-values for this interval\n",
    "        # Last y-value from each array that is <= x_left\n",
    "        y1_value = step_function(x1, y1, x_left)\n",
    "        y2_value = step_function(x2, y2, x_left)\n",
    "\n",
    "        # Compute squared difference and add to total integral\n",
    "        squared_difference = (y1_value - y2_value) ** 2\n",
    "        total_integral += squared_difference * interval_length\n",
    "\n",
    "    return total_integral\n",
    "\n",
    "def squared_difference_integral_approx(x1, y1, x2, y2, N = 1000):\n",
    "    x_points = torch.linspace(0,1,N)\n",
    "    step1 = torch.zeros_like(x_points)\n",
    "    step2 = torch.zeros_like(x_points)\n",
    "    \n",
    "    for i in range(len(x_points)):\n",
    "        step1[i] = step_function(x1, y1, x_points[i])\n",
    "        step2[i] = step_function(x2, y2, x_points[i])\n",
    "        \n",
    "    return torch.sum((step1 - step2) ** 2) / N\n",
    "\n",
    "x1 = torch.tensor([0, 0.25, 0.5, 0.75, 1]).requires_grad_(True)\n",
    "y1 = torch.tensor([0, 0.25, 0.5, 0.75, 1]).requires_grad_(True)\n",
    "x2 = torch.tensor([0, 0.5, 1]).requires_grad_(True)\n",
    "y2 = torch.tensor([0, 0.5, 1]).requires_grad_(True)\n",
    "\n",
    "diff = squared_difference_integral(x1, y1, x2, y2)\n",
    "print(diff)\n",
    "\n",
    "x1 = torch.tensor([0, 0.25, 0.5, 0.75, 1]).requires_grad_(True)\n",
    "y1 = torch.tensor([0, 0.25, 0.5, 0.75, 1]).requires_grad_(True)\n",
    "x2 = torch.tensor([0, 0.5, 1]).requires_grad_(True)\n",
    "y2 = torch.tensor([0, 0.5, 1]).requires_grad_(True)\n",
    "\n",
    "diff2 = squared_difference_integral_approx(x1, y1, x2, y2)\n",
    "print(diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2.backward() # works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3125, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = expectation(barycenter)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "the derivative for 'unique_consecutive' is not implemented.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llmsim/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llmsim/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llmsim/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: the derivative for 'unique_consecutive' is not implemented."
     ]
    }
   ],
   "source": [
    "mean.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calculating the average directly, we could just approximate it with a lot of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_step_function_approx(step_functions):\n",
    "    x = torch.linspace(0,1,10000)\n",
    "\n",
    "    # Loop through each interval\n",
    "    for i in range(len(all_x) - 1):\n",
    "        x_left = all_x[i]\n",
    "        \n",
    "        # For each step function, get the y-value at the start of this interval\n",
    "        y_values = torch.zeros(len(step_functions))\n",
    "        for j, (x, y) in enumerate(step_functions):\n",
    "            y_val = step_function(x, y, x_left)\n",
    "            y_values[j] += y_val\n",
    "        # Compute the average y-value for this interval\n",
    "        avg_y[i] += torch.mean(y_values)\n",
    "\n",
    "    # Add the final y-value after the last x-point\n",
    "    y_values = torch.zeros(len(step_functions))\n",
    "    for j, (x, y) in enumerate(step_functions):\n",
    "        y_val = y[torch.searchsorted(x, all_x[-1], right=True) - 1]\n",
    "        y_values[j] += y_val\n",
    "    # Compute the average y-value for this interval\n",
    "    avg_y[-1] += torch.mean(y_values)\n",
    "\n",
    "    return all_x, avg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,1,10000)\n",
    "y = torch.sin(2 * np.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try using SPSA to estimate the gradients since the gradients are not passing through the `torch.unique` and `torch.searchsorted` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
